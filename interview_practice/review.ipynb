{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv', index_col='col1', parse_dates=['col2'], na_values=['N/A']) # read from csv\n",
    "df = pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]}) # create from dictionary\n",
    "\n",
    "pd.concat([df1, df2], axis=0) # concatenate rows\n",
    "pd.merge(df1, df2, on='col1', how='inner') # merge by column\n",
    "\n",
    "df.set_index('col1', inplace=True) # set index\n",
    "\n",
    "df.rename(columns={'col1': 'new_col1', 'col2': 'new_col2'}, inplace=True) # rename columns\n",
    "\n",
    "df.drop(['col1', 'col2'], axis=1, inplace=True) # drop columns\n",
    "df.drop(df.columns[1], axis=1) # drop unnamed columns\n",
    "df.drop([0, 1], axis=0, inplace=True) # drop rows\n",
    "\n",
    "df.duplicated() # check for duplicate rows\n",
    "df.drop_duplicates(inplace=True) # drop duplicate rows\n",
    "df.drop_duplicates(subset=['col1', 'col2'], keep='last', inplace=True) # drop rows with duplicate values in specified columns, keep last occurrence\n",
    "\n",
    "df.head() # first 5 rows\n",
    "df.tail() # last 5 rows\n",
    "df.sample() # random row\n",
    "df.sample(10) # 10 random rows\n",
    "\n",
    "df.shape # (rows, columns)\n",
    "df.columns # column names\n",
    "df.index # row names\n",
    "\n",
    "df.info() # column names, data types, memory usage\n",
    "df.describe() # summary statistics for numeric columns\n",
    "\n",
    "df['col1'] # get a column\n",
    "df[['col1', 'col2']] # get multiple columns\n",
    "df[['col1', 'col2']].values # get a column as a numpy array\n",
    "\n",
    "df.iloc[0] # get a row by index\n",
    "df.iloc[0:5] # get multiple rows by index\n",
    "df.iloc[0:5, 0:2] # get multiple rows and columns by index\n",
    "df.iloc[0:5, 0:2].values # get multiple rows and columns by index as a numpy array\n",
    "\n",
    "df.loc[0] # get a row by index\n",
    "df.loc[0:5] # get multiple rows by index\n",
    "df.loc[0:5, 'col1':'col2'] # get multiple rows and columns by index\n",
    "df.loc[0:5, ['col1', 'col2']] # get multiple rows and columns by index\n",
    "df.loc[0:5, 'col1':'col2'].values # get multiple rows and columns by index as a numpy array\n",
    "\n",
    "df[df.col1 > 0] # filter rows\n",
    "df[(df.col1 > 0) & (df.col2 < 0)] # filter rows\n",
    "df[(df.col1 > 0) | (df.col2 < 0)] # filter rows\n",
    "df.query('col1 > 0') # filter rows\n",
    "df.query('col1 > 0 & col2 < 0') # filter rows\n",
    "df.query('col1 > 0 | col2 < 0') # filter rows\n",
    "\n",
    "df.groupby(['col1', 'col2'])['col3'].mean() # group by multiple columns and aggregate\n",
    "df.groupby('col1')['col2'].agg(['mean', 'count']) # group by column and aggregate\n",
    "df.groupby('col1').agg({'col2': 'mean', 'col3': 'count'}) # group by column and aggregate\n",
    "\n",
    "df.sort_values('col1', ascending=False) # sort by column\n",
    "df.sort_values(['col1', 'col2'], ascending=[False, True]) # sort by multiple columns\n",
    "df.sort_index(ascending=False) # sort by index\n",
    "\n",
    "df['col1'].value_counts(normalize=True, sort=False) # count unique values as percentages, don't sort\n",
    "\n",
    "df['col1'].unique() # get unique values\n",
    "df['col1'].nunique() # count unique values\n",
    "\n",
    "df['col1'].isna() # check for null values\n",
    "\n",
    "df['col1'].fillna(0) # fill null values\n",
    "df['col1'].fillna(method=...) # fill null values\n",
    "\n",
    "df['col1'].astype('int') # convert data type\n",
    "df['col1'].astype('float') # convert data type\n",
    "df['col1'].astype('str') # convert data type\n",
    "df['col1'].astype('category') # convert data type\n",
    "\n",
    "df['col1'].replace(0, 1) # replace values\n",
    "df['col1'].replace({0: 1, 1: 0}) # replace values\n",
    "\n",
    "df['col1'].apply(lambda x: x * 2) # apply a function\n",
    "df['col1'].apply(lambda x: x * 2 if x > 0 else x / 2) # apply a function\n",
    "\n",
    "df['col1'].map({0: 'zero', 1: 'one'}) # map values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(df.drop('target', axis=1)) # fit scaler and transform data\n",
    "\n",
    "# in two steps\n",
    "scaler.fit(df[['col1', 'col2']]) # fit scaler to data\n",
    "df[['col1', 'col2']] = scaler.transform(df[['col1', 'col2']]) # transform data\n",
    "\n",
    "# swap in RobustScaler for data with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier clipping using 1.5 * IQR rule\n",
    "q1 = df['col1'].quantile(0.25)\n",
    "q3 = df['col1'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "df['col1'].clip(lower=q1 - 1.5 * iqr, upper=q3 + 1.5 * iqr, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binninng with equal frequency\n",
    "df['col1'] = pd.qcut(df['col1'], q=10, labels=False, duplicates='drop')\n",
    "\n",
    "# binninng with equal width\n",
    "df['col1'] = pd.cut(df['col1'], bins=10, labels=False, duplicates='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iputing strategies\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# more accurate but slower\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "features_imputed = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding features\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# encoding targets\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "features_encoded = encoder.fit_transform(df[['col1', 'col2']])\n",
    "encoder.classes_\n",
    "features_decoded = encoder.inverse_transform(features_encoded)\n",
    "\n",
    "# ordinal encoding can also be performed by using .replace()\n",
    "encoding = {'col1': {'a': 0, 'b': 1, 'c': 2}}\n",
    "df.replace(encoding, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with strings\n",
    "\n",
    "df['col1'].str.lower() # convert to lowercase\n",
    "df['col1'].str.upper() # convert to uppercase\n",
    "df['col1'].str.strip() # strip whitespace\n",
    "df['col1'].str.replace('old', 'new') # replace values\n",
    "df['col1'].str.split(' ') # split strings\n",
    "df['col1'].str.contains('abc') # check for substring\n",
    "df['col1'].str.startswith('abc') # check for substring\n",
    "df['col1'].str.endswith('abc') # check for substring\n",
    "df['col1'].str.extract('(\\d+)', expand=False) # extract substring\n",
    "df['col1'].str.findall('(\\d+)').str[0] # find all substrings\n",
    "df['col1'].str.count('abc') # count occurrences of substring\n",
    "df['col1'].str.cat(sep=',') # concatenate strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('dogs')\n",
    "[token.lemma_.lower().strip() for token in nlp(doc) if (not token.is_stop) \n",
    "                                                   and (not token.is_punct)\n",
    "                                                   and (not token.is_digit)\n",
    "                                                   and (not token.like_num)\n",
    "                                                   and (token.lemma_.strip()!=\"\") \n",
    "                                                   and (len(token.lemma_.strip())>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_2gram = CountVectorizer(tokenizer=word_tokenize, ngram_range=(1,2), stop_words=\"english\", vocabulary=['dog', 'cat', 'monkey'])\n",
    "bag = count_2gram.fit_transform(['dog ate homework', 'monkey ate banana'])\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', ngram_range=(1,2), max_df=0.5, min_df=3)\n",
    "feature_matrix = tfidf.fit_transform(['dog ate homework', 'monkey ate banana'])\n",
    "\n",
    "# Nearest Neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "dtm = pd.DataFrame(feature_matrix.todense(), columns=tfidf.get_feature_names_out())\n",
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "nn.fit(dtm)\n",
    "n_dist, n_index = nn.kneighbors(tfidf.transform(['ball bounce']))\n",
    "for i in n_index:\n",
    "    print(df['text'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df['col1'], format='%d-%m-%Y %I:%M %p') # convert to datetime\n",
    "\n",
    "df['col1'].dt.year # get year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2) # specify n_feratures to keep\n",
    "\n",
    "pca = PCA(n_components=0.95) # specify variance to keep\n",
    "\n",
    "features_pca = pca.fit_transform(df)\n",
    "\n",
    "# for components that are not lnearly independent, use KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_classif, f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "model = regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "regressor = Ridge(alpha=0.5) # punish large coefficients\n",
    "regressor = Lasso(alpha=0.5) # shrink coefficients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "\n",
    "model = classifier.fit(X_train, y_train)\n",
    "\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "classifier = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "model = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "\n",
    "nn.fit(X_train)\n",
    "\n",
    "distances, indices = nn.kneighbors(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "y_pred = classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc)\n",
    "\n",
    "roc_display.plot()\n",
    "\n",
    "\n",
    "### Alternatively ###\n",
    "\n",
    "RocCurveDisplay.from_estimator(classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=classifier.classes_)\n",
    "\n",
    "\n",
    "### Alternatively ###\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "display = PartialDependenceDisplay.from_estimator(model, X_test, features=['col1', 'col2'])\n",
    "\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "perm_df = pd.DataFrame({'feature': X_test.columns, 'importance': perm.importances_mean})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "kmeans.fit(features)\n",
    "\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col_trans = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['col1', 'col2']),\n",
    "    ('scale', StandardScaler(), ['col3', 'col4'])], \n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(cols=['col1', 'col2']),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "### alternatively ###\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler(cols=['col1', 'col2'])),\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {'classifier__n_neighbors': [1, 5, 10]}\n",
    "\n",
    "clf = GridSearchCV(model_pipeline, grid, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "model = clf.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "pipeline = make_pipeline(scaler, classifier)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                            features, # Feature matrix\n",
    "                            target, # Target vector\n",
    "                            cv=kf, # Performance metric\n",
    "                            scoring=\"accuracy\", # Loss function\n",
    "                            n_jobs=-1) # Use all CPU cores\n",
    "\n",
    "cv_results.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt-nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
